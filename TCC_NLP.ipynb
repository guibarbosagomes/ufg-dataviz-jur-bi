{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o modelo de NLP em português\n",
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas auxiliares para validação\n",
    "siglas_estados = {\"AC\", \"AL\", \"AP\", \"AM\", \"BA\", \"CE\", \"DF\", \"ES\", \"GO\", \"MA\", \"MT\", \"MS\", \"MG\", \"PA\", \"PB\", \"PR\", \"PE\", \"PI\", \"RJ\", \"RN\", \"RS\", \"RO\", \"RR\", \"SC\", \"SP\", \"SE\", \"TO\"}\n",
    "organizacoes_keywords = [\"Tribunal\", \"Ministério\", \"Banco\", \"Procuradoria\", \"Universidade\", \"Secretaria\", \"Assembleia\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_txt(txt_path):\n",
    "    \"\"\"Lê o conteúdo de um arquivo TXT.\"\"\"\n",
    "    try:\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ')  # Remove quebras de linha\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(text):\n",
    "    \"\"\"Extrai uma frase representativa do início do texto como título.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        return sent.text.strip()\n",
    "    return \"Título Desconhecido\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text, top_n=10):\n",
    "    \"\"\"Extrai palavras-chave do texto baseando-se na frequência e relevância.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    palavras_relevantes = [token.text.lower() for token in doc if token.is_alpha and token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"]]\n",
    "    palavras_mais_frequentes = Counter(palavras_relevantes).most_common(top_n)\n",
    "    return palavras_mais_frequentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_entidades(lista, tipo):\n",
    "    \"\"\"Remove entidades curtas, irrelevantes ou que parecem ruído com técnicas heurísticas.\"\"\"\n",
    "    resultado = []\n",
    "    for item in lista:\n",
    "        item_limpo = item.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        if len(item_limpo) < 3:\n",
    "            continue\n",
    "        if tipo == \"PESSOA\":\n",
    "            if len(item_limpo.split()) < 2:\n",
    "                continue\n",
    "            if not all(p[0].isupper() for p in item_limpo.split()):\n",
    "                continue\n",
    "        elif tipo == \"ORGANIZACAO\":\n",
    "            if len(item_limpo.split()) < 2 and not any(kw.lower() in item_limpo.lower() for kw in organizacoes_keywords):\n",
    "                continue\n",
    "        elif tipo == \"LOCAL\":\n",
    "            if item_limpo.upper() not in siglas_estados and len(item_limpo.split()) < 2:\n",
    "                continue\n",
    "        elif tipo == \"LEI\":\n",
    "            if item_limpo.lower().strip() == \"lei\":\n",
    "                continue\n",
    "        resultado.append(item_limpo)\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    doc = nlp(text)\n",
    "    titulo = extract_title(text)\n",
    "    palavras_chave = extract_keywords(text)\n",
    "\n",
    "    num_palavras = len([token.text for token in doc if token.is_alpha])\n",
    "    num_caracteres = len(text)\n",
    "    num_sentencas = len(list(doc.sents))\n",
    "    num_palavras_unicas = len(set(token.text.lower() for token in doc if token.is_alpha))\n",
    "    densidade_lexical = num_palavras_unicas / num_palavras if num_palavras > 0 else 0\n",
    "\n",
    "    contagem_classes = {\n",
    "        \"Verbos\": len([token for token in doc if token.pos_ == \"VERB\"]),\n",
    "        \"Substantivos\": len([token for token in doc if token.pos_ == \"NOUN\"]),\n",
    "        \"Adjetivos\": len([token for token in doc if token.pos_ == \"ADJ\"]),\n",
    "        \"Pronomes\": len([token for token in doc if token.pos_ == \"PRON\"]),\n",
    "        \"Numerais\": len([token for token in doc if token.pos_ == \"NUM\"]),\n",
    "    }\n",
    "\n",
    "    entidades = {\"PESSOA\": [], \"ORGANIZACAO\": [], \"LOCAL\": [], \"DATA\": [], \"MONEY\": [], \"LEI\": []}\n",
    "    entidades_encontradas = []\n",
    "    for ent in doc.ents:\n",
    "        label = ent.label_\n",
    "        if label == \"PER\":\n",
    "            entidades[\"PESSOA\"].append(ent.text)\n",
    "        elif label == \"ORG\":\n",
    "            entidades[\"ORGANIZACAO\"].append(ent.text)\n",
    "        elif label == \"LOC\":\n",
    "            entidades[\"LOCAL\"].append(ent.text)\n",
    "        elif label == \"DATE\":\n",
    "            entidades[\"DATA\"].append(ent.text)\n",
    "        elif label == \"MONEY\":\n",
    "            entidades[\"MONEY\"].append(ent.text)\n",
    "        elif re.search(r\"art(igo)?\\.?(\\s+\\d+)?|lei\\s+\\d+\", ent.text.lower()):\n",
    "            entidades[\"LEI\"].append(ent.text)\n",
    "        entidades_encontradas.append((ent.text.strip(), label))\n",
    "\n",
    "    for tipo in [\"PESSOA\", \"ORGANIZACAO\", \"LOCAL\", \"LEI\"]:\n",
    "        entidades[tipo] = limpar_entidades(entidades[tipo], tipo)\n",
    "\n",
    "    df_resumo = pd.DataFrame({\n",
    "        \"Métrica\": [\"Título do Texto\", \"Número de Palavras\", \"Número de Caracteres\", \"Número de Sentenças\", \"Palavras Únicas\", \"Densidade Lexical\", \"Verbos\", \"Substantivos\", \"Adjetivos\", \"Pronomes\", \"Numerais\"],\n",
    "        \"Valor\": [titulo, num_palavras, num_caracteres, num_sentencas, num_palavras_unicas, densidade_lexical,\n",
    "                  contagem_classes[\"Verbos\"], contagem_classes[\"Substantivos\"], contagem_classes[\"Adjetivos\"],\n",
    "                  contagem_classes[\"Pronomes\"], contagem_classes[\"Numerais\"]]\n",
    "    })\n",
    "\n",
    "    df_palavras_chave = pd.DataFrame(palavras_chave, columns=[\"Palavra\", \"Frequência\"])\n",
    "    df_entidades_encontradas = pd.DataFrame(entidades_encontradas, columns=[\"Entidade\", \"Tipo\"])\n",
    "    df_entidades = {chave: pd.DataFrame(Counter(valores).items(), columns=[\"Entidade\", \"Frequência\"]) for chave, valores in entidades.items() if valores}\n",
    "\n",
    "    return df_resumo, df_entidades, df_entidades_encontradas, df_palavras_chave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder_path, output_file):\n",
    "    todos_resumos = []\n",
    "    todas_entidades_encontradas = []\n",
    "    todas_palavras_chave = []\n",
    "    entidades_agrupadas = {}\n",
    "\n",
    "    for nome_arquivo in os.listdir(folder_path):\n",
    "        if nome_arquivo.lower().endswith(\".txt\"):\n",
    "            caminho_arquivo = os.path.join(folder_path, nome_arquivo)\n",
    "            print(f\"Processando: {caminho_arquivo}\")\n",
    "            texto = extract_text_from_txt(caminho_arquivo)\n",
    "            if not texto:\n",
    "                continue\n",
    "            df_resumo, df_entidades, df_entidades_encontradas, df_palavras_chave = analyze_text(texto)\n",
    "\n",
    "            df_resumo.insert(0, \"Arquivo\", nome_arquivo)\n",
    "            df_entidades_encontradas.insert(0, \"Arquivo\", nome_arquivo)\n",
    "            df_palavras_chave.insert(0, \"Arquivo\", nome_arquivo)\n",
    "\n",
    "            todos_resumos.append(df_resumo)\n",
    "            todas_entidades_encontradas.append(df_entidades_encontradas)\n",
    "            todas_palavras_chave.append(df_palavras_chave)\n",
    "\n",
    "            for tipo, df in df_entidades.items():\n",
    "                if tipo not in entidades_agrupadas:\n",
    "                    entidades_agrupadas[tipo] = []\n",
    "                df.insert(0, \"Arquivo\", nome_arquivo)\n",
    "                entidades_agrupadas[tipo].append(df)\n",
    "\n",
    "    with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n",
    "        pd.concat(todos_resumos).to_excel(writer, sheet_name=\"Resumo\", index=False)\n",
    "        pd.concat(todas_palavras_chave).to_excel(writer, sheet_name=\"Palavras-chave\", index=False)\n",
    "        pd.concat(todas_entidades_encontradas).to_excel(writer, sheet_name=\"Entidades Encontradas\", index=False)\n",
    "        for tipo, lista_dfs in entidades_agrupadas.items():\n",
    "            pd.concat(lista_dfs).to_excel(writer, sheet_name=tipo, index=False)\n",
    "\n",
    "    print(f\"Consolidação concluída! Arquivo salvo em {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os caminhos com o formato raw para evitar problemas com as barras invertidas\n",
    "pasta_textos = r\"C:\\Users\\vitor\\OneDrive\\Área de Trabalho\\TCC\\TCC_NLP\\j1_diarios_de_justica_stj\"\n",
    "arquivo_saida = r\"C:\\Users\\vitor\\OneDrive\\Área de Trabalho\\TCC\\TCC_NLP\\resultado_consolidado.xlsx\"\n",
    "\n",
    "process_folder(pasta_textos, arquivo_saida)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
